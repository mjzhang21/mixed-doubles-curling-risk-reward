---
title: "CSAS 2026: Behavior Classes"
author: "Alejandro Haerter"
date: "2026-01-13"
format:
  pdf:
    colorlinks: true
    linkcolor: blue
    number-sections: true
    geometry: margin=1in
    fontsize: 11pt 
---

The final plan.

# Build the analysis unit: team-end

```{python}
import pandas as pd
import numpy as np

df = pd.read_csv("../data/stones_master_1.csv")  # you already have this


# Ends is a dataframe which contains two rows per match an end; one for each team.
ends = (
    df[["match_id", "end_id", "team_id1", "team_id2"]]
    .dropna(subset=["match_id", "end_id", "team_id1", "team_id2"])
    .drop_duplicates(subset=["match_id", "end_id"])
    .reset_index(drop=True)
)
```

```{python}
# no end should have multiple different team_id1/team_id2 pairs
dup_check = df.groupby(["match_id", "end_id"])[["team_id1", "team_id2"]].nunique()
bad_ends = dup_check[(dup_check["team_id1"] > 1) | (dup_check["team_id2"] > 1)]
if len(bad_ends):
    raise ValueError(f"Found ends with inconsistent team_id1/team_id2:\n{bad_ends.head()}")
```

passes quality control.

```{python}
# ends already has: match_id, end_id, team_id1, team_id2

team_end_a = ends.copy()
team_end_a["team_id"] = team_end_a["team_id1"]
team_end_a["opp_team_id"] = team_end_a["team_id2"]

team_end_b = ends.copy()
team_end_b["team_id"] = team_end_b["team_id2"]
team_end_b["opp_team_id"] = team_end_b["team_id1"]

team_end = pd.concat([team_end_a, team_end_b], ignore_index=True)[
    ["match_id", "end_id", "team_id", "opp_team_id", "team_id1", "team_id2"]
]

# Checks
assert len(team_end) == 2 * len(ends)
assert (team_end["team_id"] != team_end["opp_team_id"]).all()

team_end.head()
```

this new dataframe, team_end, just serves for future assignments.

## Data

A new dataframe must be constructed which creates exactly two rows per
match_id and end_id; one for team_id1 and one for team_id2.
Each observational unit (so for each time) is intended to quantify the
decisions made by each team during the end without using every single
shot-row (too granular, useless).

The outcome is to see how much each team changed the sheet over the end,
not what did the shooter do on one shot (stone-level).

We will do this by taking the set of unique ends (match_id, end_id) and
cross-joining with both team_id_i. We keep these keys and add
opp_team_id as a new key. 

```{python}
# now we are adding information on powerplay.
pp_context = (
    df.groupby(["match_id", "end_id"], as_index=False)
      .agg(pp_value_end=("powerplay", "max"))
)

pp_context["pp_used_end"] = pp_context["pp_value_end"].fillna(0).astype(int).ne(0)
pp_context["pp_side_end"] = np.select(
    [
        pp_context["pp_value_end"].fillna(0).astype(int).eq(1),
        pp_context["pp_value_end"].fillna(0).astype(int).eq(2),
    ],
    ["right", "left"],
    default=None,
)
```


```{python}
# --- B) Team-end context (team-relative; constant within match_id,end_id,team_id) ---
# Verify constancy within team-end
team_ctx_nunique = (
    df.groupby(["match_id", "end_id", "team_id"])[["has_hammer", "pre_end_score_diff"]]
      .nunique(dropna=False)
)

# qc. check to make sure has_hammer, pre_end_score_diff don't occur twice.
bad_team_ctx = team_ctx_nunique[(team_ctx_nunique["has_hammer"] > 1) | (team_ctx_nunique["pre_end_score_diff"] > 1)]
if len(bad_team_ctx):
    raise ValueError(f"Non-constant has_hammer or pre_end_score_diff within some team-ends:\n{bad_team_ctx.head()}")

team_context = (
    df.groupby(["match_id", "end_id", "team_id"], as_index=False)
      .agg(
          has_hammer=("has_hammer", "first"),
          pre_end_score_diff=("pre_end_score_diff", "first"), #fine assumption to make because should all be the same
      )
)
```

passes qc. we can safely add the has_hammer boolean and the pre_end_score_diff. 

```{python}
# --- C) Merge onto team_end ---
team_end = team_end.merge(
    team_context,
    on=["match_id", "end_id", "team_id"],
    how="left",
    validate="one_to_one",
)

team_end = team_end.merge(
    pp_context[["match_id", "end_id", "pp_value_end", "pp_used_end", "pp_side_end"]],
    on=["match_id", "end_id"],
    how="left",
    validate="many_to_one",
)

# Quick check
team_end[["has_hammer", "pre_end_score_diff", "pp_value_end", "pp_used_end", "pp_side_end"]].head()
```

# Attach some coordinate information

So right now we are going to use two pictures of the board. the one for shotid 9 and shotid 22.
Why? because shotid is the end of the mfgz so we can see how behavior diverges from the 
typical race to the button.

This analysis will measure net late-phase change (not necessarily how). This is a first-pass.
We select rows were shotid is 9 and 22 irregardless of hammer team, etc. We will add the
coordiantes for each stone to the team-end rows here.

```{python}
stone_cols = []
for i in range(1, 13):
    stone_cols += [f"stone_{i}_x", f"stone_{i}_y"]

missing = [c for c in stone_cols if c not in df.columns]
if missing:
    raise KeyError(f"Missing expected stone coordinate columns: {missing[:10]}")
```

There are no missing columns. This is because when ends are conceded, the ShotID rows are
still included in the data, but the coordinates are all filled with NA and the descripters
are filled with placeholder -1.

This produces the relevant dataframes to meet the "picture" goal.

```{python}
# state 9 dataframe
state9 = (
    df.loc[df["shot_id"] == 9, ["match_id", "end_id"] + stone_cols]
      .drop_duplicates(subset=["match_id", "end_id"])
      .rename(columns={c: f"{c}_s9" for c in stone_cols})
)

#state22 dataframe
state22 = (
    df.loc[df["shot_id"] == 22, ["match_id", "end_id"] + stone_cols]
      .drop_duplicates(subset=["match_id", "end_id"])
      .rename(columns={c: f"{c}_s22" for c in stone_cols})
)

# more qc
# if duplicates exist (more than 1 row for same end at shot 9/22), catch it explicitly
dup9 = df.loc[df["shot_id"] == 9].duplicated(subset=["match_id", "end_id"]).sum()
dup22 = df.loc[df["shot_id"] == 22].duplicated(subset=["match_id", "end_id"]).sum()
if dup9 or dup22:
    raise ValueError(f"Duplicate snapshot rows found: shot9 dups={dup9}, shot22 dups={dup22}")
```

More quality control just to make sure every end which contains state 9 also has state 22.

```{python}
# complete-ends filter
ends_all = team_end[["match_id", "end_id"]].drop_duplicates()
ends_with9 = state9[["match_id", "end_id"]].drop_duplicates()
ends_with22 = state22[["match_id", "end_id"]].drop_duplicates()

ends_kept = ends_all.merge(ends_with9, on=["match_id","end_id"], how="inner") \
                    .merge(ends_with22, on=["match_id","end_id"], how="inner")
ends_dropped = ends_all.merge(ends_kept, on=["match_id","end_id"], how="left", indicator=True)
ends_dropped = ends_dropped[ends_dropped["_merge"] == "left_only"].drop(columns="_merge")

drop_rate = len(ends_dropped) / len(ends_all)
print(f"Ends total: {len(ends_all)} | kept: {len(ends_kept)} | dropped: {len(ends_dropped)} | drop rate: {drop_rate:.3%}")
```

All ends are present, but we must later be careful of conceded ends impacting the data.

Final merger:

```{python}
# merger on team_ends
team_end_kept = team_end.merge(ends_kept, on=["match_id","end_id"], how="inner", validate="many_to_one")

team_end_kept = team_end_kept.merge(state9, on=["match_id","end_id"], how="left", validate="many_to_one")
team_end_kept = team_end_kept.merge(state22, on=["match_id","end_id"], how="left", validate="many_to_one")

# Sanity: no missing snapshot cols after merge
snap_cols = [f"{c}_s9" for c in stone_cols] + [f"{c}_s22" for c in stone_cols]
miss_snap = team_end_kept[snap_cols].isna().mean().mean()
print(f"Mean missingness across snapshot coordinate columns: {miss_snap:.6f}")

team_end_kept.head()
```

We find that 1.7% of coordinate data is missing from the snapshot columns. This is because of
conceded ends.

