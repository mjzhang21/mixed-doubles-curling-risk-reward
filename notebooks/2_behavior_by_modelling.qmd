---
title: "CSAS 2026: Behavior Classes"
author: "Alejandro Haerter"
date: "2026-01-13"
format:
  pdf:
    colorlinks: true
    linkcolor: blue
    number-sections: true
    geometry: margin=1in
    fontsize: 11pt 
---

The final plan.

# Build the analysis unit: team-end

```{python}
import pandas as pd
import numpy as np

df = pd.read_csv("../data/stones_master_1.csv") 

#this is a patch necessitated by section 3. these were 2 ends with mistaken first shot values.
df.loc[(df["match_id"] == "0_13_1") & (df["end_id"] == 7) & (df["shot_id"] == 7), "stone_2_y"] = 0
df.loc[(df["match_id"] == "0_8_4") & (df["end_id"] == 2) & (df["shot_id"] == 7), "stone_8_x"] = 0

# Ends is a dataframe which contains two rows per match an end; one for each team.
ends = (
    df[["match_id", "end_id", "team_id1", "team_id2"]]
    .dropna(subset=["match_id", "end_id", "team_id1", "team_id2"])
    .drop_duplicates(subset=["match_id", "end_id"])
    .reset_index(drop=True)
)
```

```{python}
# no end should have multiple different team_id1/team_id2 pairs
dup_check = df.groupby(["match_id", "end_id"])[["team_id1", "team_id2"]].nunique()
bad_ends = dup_check[(dup_check["team_id1"] > 1) | (dup_check["team_id2"] > 1)]
if len(bad_ends):
    raise ValueError(f"Found ends with inconsistent team_id1/team_id2:\n{bad_ends.head()}")
```

passes quality control.

```{python}
# ends already has: match_id, end_id, team_id1, team_id2

team_end_a = ends.copy()
team_end_a["team_id"] = team_end_a["team_id1"]
team_end_a["opp_team_id"] = team_end_a["team_id2"]

team_end_b = ends.copy()
team_end_b["team_id"] = team_end_b["team_id2"]
team_end_b["opp_team_id"] = team_end_b["team_id1"]

team_end = pd.concat([team_end_a, team_end_b], ignore_index=True)[
    ["match_id", "end_id", "team_id", "opp_team_id", "team_id1", "team_id2"]
]

# Checks
assert len(team_end) == 2 * len(ends)
assert (team_end["team_id"] != team_end["opp_team_id"]).all()

team_end.head()
```

this new dataframe, team_end, just serves for future assignments.

## Data

A new dataframe must be constructed which creates exactly two rows per
match_id and end_id; one for team_id1 and one for team_id2.
Each observational unit (so for each time) is intended to quantify the
decisions made by each team during the end without using every single
shot-row (too granular, useless).

The outcome is to see how much each team changed the sheet over the end,
not what did the shooter do on one shot (stone-level).

We will do this by taking the set of unique ends (match_id, end_id) and
cross-joining with both team_id_i. We keep these keys and add
opp_team_id as a new key. 

```{python}
# now we are adding information on powerplay.
pp_context = (
    df.groupby(["match_id", "end_id"], as_index=False)
      .agg(pp_value_end=("powerplay", "max"))
)

pp_context["pp_used_end"] = pp_context["pp_value_end"].fillna(0).astype(int).ne(0)
pp_context["pp_side_end"] = np.select(
    [
        pp_context["pp_value_end"].fillna(0).astype(int).eq(1),
        pp_context["pp_value_end"].fillna(0).astype(int).eq(2),
    ],
    ["right", "left"],
    default=None,
)
```


```{python}
# --- B) Team-end context (team-relative; constant within match_id,end_id,team_id) ---
# Verify constancy within team-end
team_ctx_nunique = (
    df.groupby(["match_id", "end_id", "team_id"])[["has_hammer", "pre_end_score_diff"]]
      .nunique(dropna=False)
)

# qc. check to make sure has_hammer, pre_end_score_diff don't occur twice.
bad_team_ctx = team_ctx_nunique[(team_ctx_nunique["has_hammer"] > 1) | (team_ctx_nunique["pre_end_score_diff"] > 1)]
if len(bad_team_ctx):
    raise ValueError(f"Non-constant has_hammer or pre_end_score_diff within some team-ends:\n{bad_team_ctx.head()}")

team_context = (
    df.groupby(["match_id", "end_id", "team_id"], as_index=False)
      .agg(
          has_hammer=("has_hammer", "first"),
          pre_end_score_diff=("pre_end_score_diff", "first"), #fine assumption to make because should all be the same
      )
)
```

passes qc. we can safely add the has_hammer boolean and the pre_end_score_diff. 

```{python}
# --- C) Merge onto team_end ---
team_end = team_end.merge(
    team_context,
    on=["match_id", "end_id", "team_id"],
    how="left",
    validate="one_to_one",
)

team_end = team_end.merge(
    pp_context[["match_id", "end_id", "pp_value_end", "pp_used_end", "pp_side_end"]],
    on=["match_id", "end_id"],
    how="left",
    validate="many_to_one",
)

# Quick check
team_end[["has_hammer", "pre_end_score_diff", "pp_value_end", "pp_used_end", "pp_side_end"]].head()
```

# Attach some coordinate information

So right now we are going to use two pictures of the board. the one for shotid 9 and shotid 22.
Why? because shotid is the end of the mfgz so we can see how behavior diverges from the 
typical race to the button.

This analysis will measure net late-phase change (not necessarily how). This is a first-pass.
We select rows were shotid is 9 and 22 irregardless of hammer team, etc. We will add the
coordiantes for each stone to the team-end rows here.

```{python}
stone_cols = []
for i in range(1, 13):
    stone_cols += [f"stone_{i}_x", f"stone_{i}_y"]

missing = [c for c in stone_cols if c not in df.columns]
if missing:
    raise KeyError(f"Missing expected stone coordinate columns: {missing[:10]}")
```

There are no missing columns. This is because when ends are conceded, the ShotID rows are
still included in the data, but the coordinates are all filled with NA and the descripters
are filled with placeholder -1.

This produces the relevant dataframes to meet the "picture" goal.

```{python}
# state 9 dataframe
state9 = (
    df.loc[df["shot_id"] == 9, ["match_id", "end_id"] + stone_cols]
      .drop_duplicates(subset=["match_id", "end_id"])
      .rename(columns={c: f"{c}_s9" for c in stone_cols})
)

#state22 dataframe
state22 = (
    df.loc[df["shot_id"] == 22, ["match_id", "end_id"] + stone_cols]
      .drop_duplicates(subset=["match_id", "end_id"])
      .rename(columns={c: f"{c}_s22" for c in stone_cols})
)

# more qc
# if duplicates exist (more than 1 row for same end at shot 9/22), catch it explicitly
dup9 = df.loc[df["shot_id"] == 9].duplicated(subset=["match_id", "end_id"]).sum()
dup22 = df.loc[df["shot_id"] == 22].duplicated(subset=["match_id", "end_id"]).sum()
if dup9 or dup22:
    raise ValueError(f"Duplicate snapshot rows found: shot9 dups={dup9}, shot22 dups={dup22}")
```

More quality control just to make sure every end which contains state 9 also has state 22.

```{python}
# complete-ends filter
ends_all = team_end[["match_id", "end_id"]].drop_duplicates()
ends_with9 = state9[["match_id", "end_id"]].drop_duplicates()
ends_with22 = state22[["match_id", "end_id"]].drop_duplicates()

ends_kept = ends_all.merge(ends_with9, on=["match_id","end_id"], how="inner") \
                    .merge(ends_with22, on=["match_id","end_id"], how="inner")
ends_dropped = ends_all.merge(ends_kept, on=["match_id","end_id"], how="left", indicator=True)
ends_dropped = ends_dropped[ends_dropped["_merge"] == "left_only"].drop(columns="_merge")

drop_rate = len(ends_dropped) / len(ends_all)
print(f"Ends total: {len(ends_all)} | kept: {len(ends_kept)} | dropped: {len(ends_dropped)} | drop rate: {drop_rate:.3%}")
```

All ends are present, but we must later be careful of conceded ends impacting the data.

Final merger:

```{python}
# merger on team_ends
team_end_kept = team_end.merge(ends_kept, on=["match_id","end_id"], how="inner", validate="many_to_one")

team_end_kept = team_end_kept.merge(state9, on=["match_id","end_id"], how="left", validate="many_to_one")
team_end_kept = team_end_kept.merge(state22, on=["match_id","end_id"], how="left", validate="many_to_one")

# Sanity: no missing snapshot cols after merge
snap_cols = [f"{c}_s9" for c in stone_cols] + [f"{c}_s22" for c in stone_cols]
miss_snap = team_end_kept[snap_cols].isna().mean().mean()
print(f"Mean missingness across snapshot coordinate columns: {miss_snap:.6f}")

team_end_kept.head()
```

We find that 1.7% of coordinate data is missing from the snapshot columns. This is because of
conceded ends.

# stone_i

We need to make sure each stone is correctly assigned.
Our findings are that stones 1-6 belong to TeamID 1 for the entirety of a game, and that
7-12 belong to Team ID for the entirety of the game. This is irregardless of LSFE, hammer,
etc. This is just a quirk of the data entry.

```{python}
# Assumes:
# - df has shot rows 7..22 and stone_{i}_x / stone_{i}_y columns
# - team_end_kept is your two-rows-per-end table (already filtered to ends with shot 9 and 22)

# prelim: helper
#stone_active is a function which checks whether it is in-play or off-sheet, i.e, not thrown yet. 
def stone_active(df_sub: pd.DataFrame, i: int) -> pd.Series:

    x = df_sub[f"stone_{i}_x"]
    y = df_sub[f"stone_{i}_y"]
    # Active if not (x==0 and y==0). 4095 indicates off-sheet but still "thrown/exists".
    return ~((x == 0) & (y == 0))
```

I will do an activation-based validation strategy.

```{python}
# We use stones 2 and 8 because they are non-preplaced and belong to blocks 1–6 and 7–12 respectively.
# We found mapping is fixed to team_id1 (1–6) and team_id2 (7–12) except two anomalous ends.

shots = df[df["shot_id"].between(7, 22)].copy()

# So this checks which is active first, ie the first thrown stone in an end
# via min shotid (which is 7)
# First active shot for stone_2 and stone_8 within each end
first2 = (
    shots.assign(active2=stone_active(shots, 2))
         .loc[lambda d: d["active2"]]
         .groupby(["match_id", "end_id"], as_index=False)["shot_id"]
         .min()
         .rename(columns={"shot_id": "stone2_first_shot"})
)

first8 = (
    shots.assign(active8=stone_active(shots, 8))
         .loc[lambda d: d["active8"]]
         .groupby(["match_id", "end_id"], as_index=False)["shot_id"]
         .min()
         .rename(columns={"shot_id": "stone8_first_shot"})
)
```

We do more data quality control.

```{python}
# Identify anomalies where both look active at shot 7 (or other inconsistent patterns)
# Also catch ends where either stone never becomes active (shouldn't happen in complete ends)
activation = first2.merge(first8, on=["match_id", "end_id"], how="outer", validate="one_to_one")

activation["stone2_missing"] = activation["stone2_first_shot"].isna()
activation["stone8_missing"] = activation["stone8_first_shot"].isna()

# Specific anomaly: both activate at shot 7 (can indicate bad coords / prefilled)
activation["both_active_at_7"] = (
    activation["stone2_first_shot"].eq(7) & activation["stone8_first_shot"].eq(7)
)

# Flag ends with any activation issue
activation["activation_issue"] = (
    activation["stone2_missing"] | activation["stone8_missing"] | activation["both_active_at_7"]
)

# List problematic ends
bad_ends = activation.loc[activation["activation_issue"], ["match_id", "end_id"]].dropna()

print(f"Activation issues flagged: {len(bad_ends)} ends")
```

In our data we had two bad ends. These were match 0_13_1 end 7 and 0_8_4 end 2.
Manual inspection revealed:
- The value for stone_2_y was mistakenly filled for 0_13_1 end 7; should be 0
- The value for stone_8_x was mistakenly filled for 0_8_4 end 2; should be 0
We went back and manually patched that when importing the data;
there are no more issues.

We now have to map the correct ownership back to the team_end dataset.
Filters for bad ends have been commented out because we no longer require them.

```{python}
# 2) Ownership mapping decision (fixed mapping)
# ----------------------------
# Based on your verification: team_id1 owns stones 1–6, team_id2 owns stones 7–12.
# No swap repair is applied; we only drop flagged anomalous ends.

# bad_keys = set(map(tuple, bad_ends.to_numpy())) # No longer required.

team_end_valid = team_end_kept.copy()
# mask_bad = team_end_valid[["match_id", "end_id"]].apply(tuple, axis=1).isin(bad_keys) # no longer required.
# team_end_valid = team_end_valid.loc[~mask_bad].copy()

# print(f"team_end_kept rows: {len(team_end_kept)} | after dropping bad ends: {len(team_end_valid)}")

#known_bad = {("0_8_4", 2), ("0_13_1", 7)}
#present_known = known_bad.intersection(set(map(tuple, team_end_kept[["match_id","end_id"]].drop_duplicates().to_numpy())))
#if present_known:
#    present_after = known_bad.intersection(set(map(tuple, team_end_valid[["match_id","end_id"]].drop_duplicates().to_numpy())))
#    assert len(present_after) == 0, f"Known bad ends still present: {present_after}"

# team_end_valid is what you should carry forward
```

In the end, only two ends were patched, but this section exists because of
an erroroneous assumption in the original data dictionary.