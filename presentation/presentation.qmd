---
title: "Mixed Doubles Curling: Steal classification"
author: "Mark Zhang"
format: revealjs
theme: simple
transition: none
number-sections: false
bibliography: references.bib 
execute:
  echo: false
---

## Background
::: {.nonincremental}
- teams throw stones down an ice sheet and aim to get their stones closer
  to the button
- one match has 8 ends, each end teams throw 5 stones
- first two stones are preplaced, hammer has stone in the "house"
- powerplay clears the board
- mixed doubles curling leads to more action and higher scoring ends
:::

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv('../data/merged_stones.csv')
```

## The Data
```{python}
df = pd.read_csv('../data/merged_stones.csv')
print(df.info())
```

## The Data
::: {.nonincremental}
- Beijing 2022 Olympic Winter Games, World Mixed Doubles 
  Championship 2023,2024,2025 scraped from a Results Book

- `shot_{i}_x`,`shot_{i}_y`: coordinates for board position after every shot played
- `task`: type of shot played
- `result`: points earned in the end
- `shot_id`,`end_id`,`match_id`: identifies match context
- `hammer`: identifies hammer team
:::

## Hammer vs Non-hammer points per end
```{python}
end_df = (
    df.sort_values(['match_id','end_id','shot_id'])
      .groupby(['match_id','end_id','hammer'], as_index=False)
      .agg({

          'result': 'first'       # end result
      })
)
results = sorted(end_df['result'].unique())

hammer_counts = (
    end_df[end_df['hammer'] == 1]['result']
    .value_counts()
    .reindex(results, fill_value=0)
)

nonhammer_counts = (
    end_df[end_df['hammer'] == 0]['result']
    .value_counts()
    .reindex(results, fill_value=0)
)

x = np.arange(len(results))
width = 0.35  # width of the bars

fig, ax = plt.subplots(figsize=(12,6))
ax.bar(
    x - width/2, 
    hammer_counts, 
    width=width, 
    label='Hammer', 
    color='blue'
)
ax.bar(
    x + width/2, 
    nonhammer_counts, 
    width=width, 
    label='Non-Hammer', 
    color='orange'
)

ax.set_xticks(x)
ax.set_xticklabels(results)
ax.set_xlabel('Result')
ax.set_ylabel('Number of Ends')
ax.set_title('Number of Ends by Result: Hammer vs Non-Hammer')
ax.legend()
plt.show()
```

## Hammer vs Non-hammer shot types
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Shot label mapping
task_labels = {
    0: "Draw",
    1: "Front",
    2: "Guard",
    3: "Raise / Tap-back",
    4: "Wick / Soft Peeling",
    5: "Freeze",
    6: "Take-out",
    7: "Hit and Roll",
    8: "Clearing",
    9: "Double Take-out",
    10: "Promotion Take-out",
    11: "Through",
    13: "No Statistics"
}

# Map labels
df["task_label"] = df["task"].map(task_labels).fillna("Other")

# --- HAMMER counts ---
hammer_counts = (
    df[df["hammer"] == 1]["task_label"]
    .value_counts()
    .sort_values(ascending=False)
)

# --- NON-HAMMER counts ---
nonhammer_counts = (
    df[df["hammer"] == 0]["task_label"]
    .value_counts()
    .sort_values(ascending=False)
)

# --- Plot both side-by-side ---
plt.figure(figsize=(14,6))

# Hammer plot
plt.subplot(1,2,1)
sns.barplot(x=hammer_counts.values, y=hammer_counts.index)
plt.title("Most Common Shot Types (Hammer)")
plt.xlabel("Count")
plt.ylabel("Shot Type")

# Non-Hammer plot
plt.subplot(1,2,2)
sns.barplot(x=nonhammer_counts.values, y=nonhammer_counts.index)
plt.title("Most Common Shot Types (Non-Hammer)")
plt.xlabel("Count")
plt.ylabel("")

plt.tight_layout()
plt.show()

```
## Previous Research
- Clement uses score differences and team strength differences to 
  determine hammer team strategy[@Clement2012]
- previous research doesn't address mixed doubles specifically
- no use of stone level data
  
## Steals and project overview

A steal occurs when the non-hammer team scores in an end.

**Problem**: Mixed doubles teams want strategies to steal more often
and win.

**Goal**: Identify variables from the board position and shot types
to classify whether the non-hammer team steals the end.

```{python}
df['steal'] = np.where(
                df['hammer'] == 0, 
                (df['result'] > 0).astype(int), 
                np.nan
            )
freq_table = df['steal'].value_counts().reset_index()
freq_table.columns = ['steal', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / \
                            freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
```

## Most Common Shot in End vs Steal Count
```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# --- Filter non-hammer ends that were actually stolen ---
stolen_ends = df[(df["hammer"] == 0) & (df["steal"] == 1)].copy()

# --- Take the first shot of each stolen end (optional) ---
first_shot_stolen = (
    stolen_ends
    .sort_values(['match_id','end_id','shot_id'])
    .groupby(['match_id','end_id'], as_index=False)
    .first()
)

# --- Count the mode (most common first shot type) ---
mode_counts = first_shot_stolen['task_label'].value_counts()

# --- Plot ---
plt.figure(figsize=(10,6))
sns.barplot(x=mode_counts.values, y=mode_counts.index, palette="Blues_r")
plt.title("Most Common First Shot in Stolen Ends (Non-Hammer)", fontsize=16)
plt.xlabel("Number of Ends")
plt.ylabel("First Shot Type")

# Show count labels
for i, val in enumerate(mode_counts.values):
    plt.text(val + 0.5, i, str(val), va="center")

plt.tight_layout()
plt.show()
```

## Team Steal rates
Team strength difference is a possible factor
```{python}
# Only non-hammer ends
non_hammer_ends = df[df['hammer'] == 0]

# Group by team
team_agg = non_hammer_ends.groupby(['team_id','competition_id'])['steal'].agg(
    steal_rate='mean',       # fraction of ends stolen
    total_ends='count',      # number of ends
    steals='sum'             # total steals
).reset_index()
team_agg_sorted = team_agg.sort_values(
                                by='steal_rate', 
                                ascending=False
                            ).reset_index(drop=True)
print(team_agg_sorted.head(10))
```

## Converting to End-Level Data
- outcome variable of steal only occurs at the end
- stones 1-6 belong to non hammer
- stones 7-12 belong to hammer
- aggregrated stones 2-5 and 8-11 and remove 2 rows from each end
- reduced ~26000 rows down to ~2500 rows


## Aggregrating and engineering features {.smaller}
- aggregrated non hammer and hammer stones thrown
- computed min,max,avg,std distance from stone
- draw, takeout, guard counts
- horizontal, vertical offset
- differences of most features
```{python}
df = df[df.groupby(['match_id','end_id'])['hammer'].transform('nunique') == 2]
```
```{python}
BUTTON_X, BUTTON_Y = 750, 800

# Distance to button
def dist_to_button(x, y):
    return np.sqrt((x - BUTTON_X)**2 + (y - BUTTON_Y)**2)

def compute_end_features_from_coords(df):
    features = []

    for (match_id, end_id), group in df.groupby(['match_id','end_id']):
        end_feat = {'match_id': match_id, 'end_id': end_id}

        # Identify hammer/non-hammer teams
        hammer_team = group.loc[group['hammer']==1, 'team_id'].iloc[0]
        non_hammer_team = group.loc[group['hammer']==0, 'team_id'].iloc[0]
        end_feat['hammer_team'] = hammer_team
        end_feat['non_hammer_team'] = non_hammer_team

        # ---- Non-hammer stones 2-5 ----
        nh = group[group['team_id']==non_hammer_team]
        nh_stones_x = (
            nh[['stone_2_x', 'stone_3_x', 'stone_4_x', 'stone_5_x']]
            .values
            .flatten()
        )

        nh_stones_y = (
            nh[['stone_2_y', 'stone_3_y', 'stone_4_y', 'stone_5_y']]
            .values
            .flatten()
        )
        nh_dists = dist_to_button(nh_stones_x, nh_stones_y)

        end_feat['nh_min_dist_button'] = nh_dists.min()
        end_feat['nh_max_dist_button'] = nh_dists.max()
        end_feat['nh_avg_dist_button'] = nh_dists.mean()
        end_feat['nh_std_dist_button']  = nh_dists.std()   
        end_feat['nh_stones_in_house'] = np.sum(nh_dists <= 183) 
       

        end_feat['nh_avg_horiz_offset'] = np.mean(nh_stones_x - BUTTON_X)
        end_feat['nh_avg_vert_offset'] = np.mean(nh_stones_y - BUTTON_Y)

        end_feat['nh_guard_count'] = np.sum(nh['task'] == 2)
        end_feat['nh_draw_count'] = np.sum(nh['task'] == 0)
        end_feat['nh_takeout_count'] = np.sum(nh['task'].isin([6, 9, 10]))
        end_feat['nh_raise_count']=np.sum(nh['task'] == 3)
        # ---- Hammer stones 8-11 ----
        h = group[group['team_id']==hammer_team]
        h_stones_x = (
            h[['stone_8_x', 'stone_9_x', 'stone_10_x', 'stone_11_x']]
            .values
            .flatten()
        )

        h_stones_y = (
            h[['stone_8_y', 'stone_9_y', 'stone_10_y', 'stone_11_y']]
            .values
            .flatten()
        )

        h_dists = dist_to_button(h_stones_x, h_stones_y)

        end_feat['h_min_dist_button'] = h_dists.min()
        end_feat['h_max_dist_button'] = h_dists.max()
        end_feat['h_avg_dist_button'] = h_dists.mean()
        end_feat['h_std_dist_button']  = h_dists.std()  
        end_feat['h_stones_in_house'] = np.sum(h_dists <= 183)
        

        end_feat['h_avg_horiz_offset'] = np.mean(h_stones_x - BUTTON_X)
        end_feat['h_avg_vert_offset'] = np.mean(h_stones_y - BUTTON_Y)


        end_feat['h_takeout_count'] = np.sum(h['task'].isin([6, 9, 10]))
        end_feat['h_draw_count'] = np.sum(h['task'] == 0)
        end_feat['h_guard_count'] = np.sum(h['task'] == 2)
        end_feat['h_raise_count']=np.sum(h['task'] == 3)

        end_feat['nh_stone_ratio'] = end_feat['nh_stones_in_house'] / (
        end_feat['nh_stones_in_house'] + end_feat['h_stones_in_house'] + 1e-6
        )
        end_feat['takeout_diff'] = (
            end_feat['nh_takeout_count'] - end_feat['h_takeout_count']
        )

        end_feat['draw_diff'] = (
            end_feat['nh_draw_count'] - end_feat['h_draw_count']
        )

        end_feat['guard_diff'] = (
            end_feat['nh_guard_count'] - end_feat['h_guard_count']
        )
        end_feat['raise_diff'] = (
            end_feat['nh_raise_count'] - end_feat['h_raise_count']
        )


        # Powerplay
        if 'powerplay' in group.columns:
            end_feat['powerplay'] = group['powerplay'].max()

        # Target: did non-hammer steal?

        if 'result' in group.columns:
            steal_value = (
                group.loc[group['team_id'] == non_hammer_team, 
                'result']       
            ).iloc[0] > 0

            end_feat['steal'] = int(steal_value)

        # Distance differences
        end_feat['min_dist_diff'] = (
            end_feat['nh_min_dist_button'] - end_feat['h_min_dist_button']
        )

        end_feat['avg_dist_diff'] = (
            end_feat['nh_avg_dist_button'] - end_feat['h_avg_dist_button']
        )

        end_feat['max_dist_diff'] = (
            end_feat['nh_max_dist_button'] - end_feat['h_max_dist_button']
        )

        end_feat['std_dist_diff'] = (
            end_feat['nh_std_dist_button'] - end_feat['h_std_dist_button']
        )


        features.append(end_feat)

    return pd.DataFrame(features)

# Compute features
end_features = compute_end_features_from_coords(df)
end_features.head()

```

## Logistic Regression

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    confusion_matrix,
    ConfusionMatrixDisplay
)

import matplotlib.pyplot as plt
import numpy as np


# -------------------
# Features and target
# -------------------
feature_cols = [
    # Non-hammer distances
    'nh_min_dist_button','nh_max_dist_button','nh_avg_dist_button','nh_std_dist_button',
    'nh_stones_in_house','nh_avg_horiz_offset','nh_avg_vert_offset',
    
    # Non-hammer counts
    'nh_draw_count','nh_takeout_count','nh_guard_count','nh_raise_count',
    
    # Hammer distances
    'h_min_dist_button','h_max_dist_button','h_avg_dist_button','h_std_dist_button',
    'h_stones_in_house','h_avg_horiz_offset','h_avg_vert_offset',
    
    # Hammer counts
    'h_draw_count','h_takeout_count','h_guard_count','h_raise_count',
    
    # Relative / derived
    'nh_stone_ratio','takeout_diff','draw_diff','guard_diff','raise_diff',
    'min_dist_diff','avg_dist_diff','max_dist_diff','std_dist_diff',
    
    # Tactical
    'powerplay'
]

X = end_features[feature_cols]
y = end_features['steal']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)



# -------------------
# LOGISTIC L1
# -------------------
logit = Pipeline([
    ('scaler', StandardScaler()),
    ('model', LogisticRegressionCV(
        Cs=10,
        penalty='l1',
        solver='saga',
        cv=5,
        scoring='roc_auc',
        max_iter=5000,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ))
])

logit.fit(X_train, y_train)
logit_pred = logit.predict(X_test)
logit_prob = logit.predict_proba(X_test)[:, 1]

print(classification_report(y_test, logit_pred))
print("ROC-AUC:", roc_auc_score(y_test, logit_prob))
cm = confusion_matrix(y_test, logit_pred)
print("Confusion Matrix:")
print(cm)

```

## LR - coefficients
```{python}
# Coefficients
logit_coefs = logit.named_steps['model'].coef_[0]
feat_df_logit = pd.DataFrame({
    'feature': feature_cols,
    'coef': logit_coefs
}).sort_values(by='coef', key=abs, ascending=False)

print("\nLogistic L1 Coefficients:")
print(feat_df_logit)
```

## XGBoost
```{python}
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

# Define XGBoost
xgb = XGBClassifier(
    n_estimators=250,
    max_depth=5,
    learning_rate=0.01,
    subsample=0.8,
    scale_pos_weight = (sum(y_train == 0)) / (sum(y_train == 1)),
    use_label_encoder=False,
    eval_metric='aucpr',
    random_state=42
)

# Train XGBoost
xgb.fit(X_train, y_train)

# Predictions
xgb_pred = xgb.predict(X_test)
xgb_prob = xgb.predict_proba(X_test)[:, 1]

# Evaluation
print(classification_report(y_test, xgb_pred))

roc_auc = roc_auc_score(y_test, xgb_prob)
print("ROC-AUC:", roc_auc)

cm = confusion_matrix(y_test, xgb_pred)
print("Confusion Matrix:")
print(cm)

```

## XGBoost - SHAP values
```{python}
import shap

# For XGBoost, use TreeExplainer
explainer = shap.TreeExplainer(xgb)  # your trained XGBoost model

# Compute SHAP values for the test set
shap_values = explainer.shap_values(X_test)

# shap_values is (num_samples, num_features) for binary classification
# For class 1 ("steal")
shap.summary_plot(shap_values, X_test, plot_type="dot", max_display=10)

```
## Conclusion {.smaller}
Model Performance:

- XGBoost beats logistic l1 
- minority class F1 score of .53
  
Key Features:

- `takeout_diff`,`draw_diff`,`guard_diff`,`horiz_offset`,`nh_stone_ratio`
  were most predictive of steals

Limitations:

- small sample size, class imbalance, loss of information through aggregration

## Further exploration 
- explore, engineer features, think of questions relating to powerplay
- relating certain shot sequences contributing to steal probability
- RNN, models which deal with temporal dependence
- CNN, visualizing board info as images 
- clustering to identify strategies and patterns



# References
