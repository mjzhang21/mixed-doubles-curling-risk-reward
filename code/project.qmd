


```{python}
import pandas as pd
import numpy as np

df_competition = pd.read_csv("../data/Competition.csv")
df_competitors  = pd.read_csv("../data/Competitors.csv")
df_games        = pd.read_csv("../data/Games.csv")
df_teams        = pd.read_csv("../data/Teams.csv")
df_ends         = pd.read_csv("../data/Ends.csv")
df_stones       = pd.read_csv("../data/Stones.csv")

```
Merge the ends and games data with the stones data.
```{python}

merged = pd.merge(
    df_stones,
    df_ends,
    on=["CompetitionID","SessionID","GameID","EndID","TeamID"],
    how="left"
)

df = pd.merge(
    merged,
    df_games,
    on=["CompetitionID","SessionID","GameID"],
    how="left"
)
#df = pd.merge(
    #df,
    #df_competitors,
   # on=["CompetitionID","TeamID"],
    #how="left"
#)
df.columns = df.columns.str.lower().str.replace('id', '_id')
df['match_id'] = df['competition_id'].astype(str) + '_' + df['session_id'].astype(str) + '_' + df['game_id'].astype(str)
float_cols = df.select_dtypes(include=['float64']).columns
df[float_cols] = df[float_cols].astype(np.float32)
df.head()
```
```{python}
df.info()
```
```{python}
df.isnull().sum()
```
```{python}
categorical_cols = [
    'competition_id',
    'end_id',
    'player_id',
    'task',
    'handle',
    'sheet',
    'lsfe',
    'winner',
    'powerplay',
    'timeout',
    'points'
]


for col in categorical_cols:
    freq_table = df[col].value_counts(dropna=False).reset_index()
    freq_table.columns = [col, 'Frequency']
    freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
    
    print(f"Frequency table for {col}:\n", freq_table, "\n")
```

Fill in data that is supposed to be 0.
```{python}
cols_to_fill = ['powerplay', 'timeout']
df[cols_to_fill] = df[cols_to_fill].fillna(0)
df[cols_to_fill] = df[cols_to_fill].astype('int')

```
Turn -1 into NA or invalid data.
```{python}
columns_to_fix = ['task', 'handle', 'points']

df[columns_to_fix] = df[columns_to_fix].replace(-1, pd.NA)

```
Drop rows which dont have any coordinate data.
```{python}
df = df.dropna(subset=["stone_3_x"])
df.isnull().sum()
```
```{python}
count = 0
df=df.sort_values(by="shot_id")
for (mid, eid), game in df.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","lsfe","resultstr1","resultstr2","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
Create hammer column that alternates according to curling rules.
```{python}
import pandas as pd

df = df.sort_values(["match_id"])

df["hammer"] = -1 

for mid, game in df.groupby(["match_id"]):
    game = game.sort_values("end_id")
    
    team1 = game["team_id1"].iloc[0]
    team2 = game["team_id2"].iloc[0]

    lsfe = game["lsfe"].iloc[0] 
    hammer_val = lsfe  
    
    # Loop through ends
    for eid, end in game.groupby("end_id"):
        # Assign hammer for this end
        df.loc[end.index, "hammer"] = end["team_id"].apply(
        lambda tid: hammer_val if tid == team1 else (1 - hammer_val)
    )
        
        # Compute points for each team in this end
        points_team1 = end.loc[end["team_id"] == team1, "result"].iloc[0]
        points_team2 = end.loc[end["team_id"] == team2, "result"].iloc[0]
        
        # If hammer team scored, switch hammer for next end
        if (hammer_val == 1 and points_team1 > 0) or (hammer_val == 0 and points_team2 > 0):
            hammer_val = 1 - hammer_val 

```
```{python}
count = 0
df = df.sort_values(by="shot_id")
for (mid, eid), game in df.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","lsfe","result","hammer"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
Fix invalid/ mistyped result and team id.
```{python}
shots_per_team = df.groupby(["match_id","end_id","team_id"]).size()
print(shots_per_team.describe())
print(shots_per_team[shots_per_team > 5])
cid, sid, gid, eid= 24250026, 18, 1, 9
df_invalid_shots = df[(df["competition_id"] == cid) &
            (df["session_id"] == sid) &
            (df["game_id"] == gid) &
            (df["end_id"] == eid)][["team_id","end_id","result"]]

print(df_invalid_shots)
df.loc[25375, ["team_id", "result"]] = [37, 1]
```
```{python}
import matplotlib.pyplot as plt

mean_points = df.groupby("end_id")["result"].mean()
plt.bar(mean_points.index.astype(str), mean_points.values, color='skyblue')
plt.title("Average points per end")
plt.xlabel("End")
plt.ylabel("Average Points")
plt.show()
```
```{python}
df[df["result"] >5][["match_id","end_id","result"]]
```
```{python}
df.loc[df["result"] > 5, "result"] = pd.NA
```
```{python}
mean_points = df.groupby("end_id")["result"].mean()
plt.bar(mean_points.index.astype(str), mean_points.values, color='skyblue')
plt.title("Average points per end")
plt.xlabel("End")
plt.ylabel("Average Points")
plt.show()
```
Average points per end looks more evenly spread now.

```{python}
df = df.dropna(subset=["result"])
null_counts = df.isnull().sum()
print(null_counts)
```
Check if all stone coordinates are valid
```{python}
x_cols = ['stone_1_x',
            'stone_2_x',
            'stone_3_x',
            'stone_4_x',
            'stone_5_x',
            'stone_6_x',
            'stone_7_x',
            'stone_8_x',
            'stone_9_x',
            'stone_10_x',
            'stone_11_x',
            'stone_12_x']
y_cols   = ['stone_1_y',
            'stone_2_y',
            'stone_3_y',
            'stone_4_y',
            'stone_5_y',
            'stone_6_y',
            'stone_7_y',
            'stone_8_y',
            'stone_9_y',
            'stone_10_y',
            'stone_11_y',
            'stone_12_y']  
             
x_min, x_max = 0, 1500
y_min, y_max = 0, 3000

# Create masks column by column
x_ok = (df[x_cols] >= x_min) & (df[x_cols] <= x_max) | (df[x_cols] == 4095)
y_ok = (df[y_cols] >= y_min) & (df[y_cols] <= y_max) | (df[y_cols] == 4095)

# Rows where all x and y columns are okay
rows_ok = x_ok.all(axis=1) & y_ok.all(axis=1)

# Rows that are NOT ok
rows_out_of_range = df[~rows_ok]

print(rows_out_of_range)

```
All the stone coordinates are in range


```{python}
df.info()
```
```{python}
categorical_cols = [
    'competition_id',
    'end_id',
    'player_id',
    'task',
    'handle',
    'sheet',
    'lsfe',
    'winner',
    'powerplay',
    'timeout',
    'points'
]


for col in categorical_cols:
    freq_table = df[col].value_counts(dropna=False).reset_index()
    freq_table.columns = [col, 'Frequency']
    freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
    
    print(f"Frequency table for {col}:\n", freq_table, "\n")
```
```{python}
df_invalid_player = df[(df["player_id"] == 3)][["match_id","end_id"]]
print("player_id = 3:")
print(df_invalid_player)
print("---------------------")
df_invalid_points = df[(df["points"] > 4)][["match_id","end_id"]]
print("points > 4:")
print(df_invalid_points)
```
```{python}
match_id_to_view = '22230015_48_1'
df_match = df[df["match_id"] == match_id_to_view]
df_match = df_match.sort_values(by='shot_id')
count = 0
for (mid, eid), game in df_match.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","player_id","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
```{python}
match_id_to_view = '0_9_2'
df_match = df[df["match_id"] == match_id_to_view]
df_match = df_match.sort_values(by='shot_id')
count = 0
for (mid, eid), game in df_match.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","points","result","task","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```

EXPLORATION

We will explore the different types of shots
"0": "Draw"
"1": "Front"
"2": "Guard"
"3": "Raise / Tap-back"
"4": "Wick / Soft Peeling"
"5": "Freeze"
"6": "Take-out"
"7": "Hit and Roll"
"8": "Clearing"
"9": "Double Take-out"
"10": "Promotion Take-out"
"11": "through"
"13": "no statistics"
```{python}
freq_table = df['task'].value_counts().reset_index()
freq_table.columns = ['Task', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['Task'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Task Values')
plt.xlabel('Task')
plt.ylabel('Frequency')
```

Draw is the most common shot by far.
Freeze and Through are the least common.

How often players perfectly execute their shot?
```{python}
freq_table = df['points'].value_counts().reset_index()
freq_table.columns = ['Point', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['Point'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Point Values')
plt.xlabel('Point')
plt.ylabel('Frequency')
```

Logistic Regression
Outcome Variable: Steal
```{python}
df_hammer = df[df['hammer'] == 0].copy()
end_df = df_hammer.groupby(['match_id', 'end_id', 'team_id']).agg({
    'result': 'first',   # points scored by this team in the end
    'hammer': 'first'  # which team had the hammer
}).reset_index()

end_df['steal'] = ((end_df['result'] > 0) & (end_df['hammer'] == 0)).astype(int)

if 'steal' in df_hammer.columns:
    df_hammer = df_hammer.drop(columns=['steal'])

df_hammer = df_hammer.merge(
    end_df[['match_id', 'end_id', 'team_id', 'steal']],
    on=['match_id', 'end_id', 'team_id'],
    how='left'
)

```
```{python}
freq_table = df_hammer['steal'].value_counts().reset_index()
freq_table.columns = ['steal', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['steal'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Steal Values')
plt.xlabel('Steal')
plt.ylabel('Frequency')

```



```{python}
import pandas as pd
import numpy as np

df_competition = pd.read_csv("../data/Competition.csv")
df_competitors  = pd.read_csv("../data/Competitors.csv")
df_games        = pd.read_csv("../data/Games.csv")
df_teams        = pd.read_csv("../data/Teams.csv")
df_ends         = pd.read_csv("../data/Ends.csv")
df_stones       = pd.read_csv("../data/Stones.csv")

```
Merge the ends and games data with the stones data.
```{python}

merged = pd.merge(
    df_stones,
    df_ends,
    on=["CompetitionID","SessionID","GameID","EndID","TeamID"],
    how="left"
)

df = pd.merge(
    merged,
    df_games,
    on=["CompetitionID","SessionID","GameID"],
    how="left"
)
df.columns = df.columns.str.lower().str.replace('id', '_id')
df['match_id'] = df['competition_id'].astype(str) + '_' + df['session_id'].astype(str) + '_' + df['game_id'].astype(str)
float_cols = df.select_dtypes(include=['float64']).columns
df[float_cols] = df[float_cols].astype(np.float32)
df.head()
```
```{python}
df.info()
```
```{python}
df.isnull().sum()
```
```{python}
categorical_cols = [
    'competition_id',
    'end_id',
    'player_id',
    'task',
    'handle',
    'sheet',
    'lsfe',
    'winner',
    'powerplay',
    'timeout',
    'points'
]


for col in categorical_cols:
    freq_table = df[col].value_counts(dropna=False).reset_index()
    freq_table.columns = [col, 'Frequency']
    freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
    
    print(f"Frequency table for {col}:\n", freq_table, "\n")
```

Fill in data that is supposed to be 0.
```{python}
cols_to_fill = ['powerplay', 'timeout']
df[cols_to_fill] = df[cols_to_fill].fillna(0)
df[cols_to_fill] = df[cols_to_fill].astype('int')

```
Turn -1 into NA or invalid data.
```{python}
columns_to_fix = ['task', 'handle', 'points']

df[columns_to_fix] = df[columns_to_fix].replace(-1, pd.NA)

```
Drop rows which dont have any coordinate data.
```{python}
df = df.dropna(subset=["stone_3_x"])
df.isnull().sum()
```
```{python}
count = 0
df=df.sort_values(by="shot_id")
for (mid, eid), game in df.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","lsfe","resultstr1","resultstr2","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
Create hammer column that alternates according to curling rules.
```{python}
import pandas as pd

df = df.sort_values(["match_id"])

df["hammer"] = -1 

for mid, game in df.groupby(["match_id"]):
    game = game.sort_values("end_id")
    
    team1 = game["team_id1"].iloc[0]
    team2 = game["team_id2"].iloc[0]

    lsfe = game["lsfe"].iloc[0] 
    hammer_val = lsfe  
    
    # Loop through ends
    for eid, end in game.groupby("end_id"):
        # Assign hammer for this end
        df.loc[end.index, "hammer"] = end["team_id"].apply(
        lambda tid: hammer_val if tid == team1 else (1 - hammer_val)
    )
        
        # Compute points for each team in this end
        points_team1 = end.loc[end["team_id"] == team1, "result"].iloc[0]
        points_team2 = end.loc[end["team_id"] == team2, "result"].iloc[0]
        
        # If hammer team scored, switch hammer for next end
        if (hammer_val == 1 and points_team1 > 0) or (hammer_val == 0 and points_team2 > 0):
            hammer_val = 1 - hammer_val 

```
```{python}
count = 0
df = df.sort_values(by="shot_id")
for (mid, eid), game in df.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","lsfe","result","hammer"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
Fix invalid/ mistyped result and team id.
```{python}
shots_per_team = df.groupby(["match_id","end_id","team_id"]).size()
print(shots_per_team.describe())
print(shots_per_team[shots_per_team > 5])
cid, sid, gid, eid= 24250026, 18, 1, 9
df_invalid_shots = df[(df["competition_id"] == cid) &
            (df["session_id"] == sid) &
            (df["game_id"] == gid) &
            (df["end_id"] == eid)][["team_id","end_id","result"]]

print(df_invalid_shots)
df.loc[25375, ["team_id", "result"]] = [37, 1]
```
```{python}
import matplotlib.pyplot as plt

mean_points = df.groupby("end_id")["result"].mean()
plt.bar(mean_points.index.astype(str), mean_points.values, color='skyblue')
plt.title("Average points per end")
plt.xlabel("End")
plt.ylabel("Average Points")
plt.show()
```
```{python}
df[df["result"] >5][["match_id","end_id","result"]]
```
```{python}
df.loc[df["result"] > 5, "result"] = pd.NA
```
```{python}
mean_points = df.groupby("end_id")["result"].mean()
plt.bar(mean_points.index.astype(str), mean_points.values, color='skyblue')
plt.title("Average points per end")
plt.xlabel("End")
plt.ylabel("Average Points")
plt.show()
```
Average points per end looks more evenly spread now.

```{python}
df = df.dropna(subset=["result"])
null_counts = df.isnull().sum()
print(null_counts)
```
Check if all stone coordinates are valid
```{python}
x_cols = ['stone_1_x',
            'stone_2_x',
            'stone_3_x',
            'stone_4_x',
            'stone_5_x',
            'stone_6_x',
            'stone_7_x',
            'stone_8_x',
            'stone_9_x',
            'stone_10_x',
            'stone_11_x',
            'stone_12_x']
y_cols   = ['stone_1_y',
            'stone_2_y',
            'stone_3_y',
            'stone_4_y',
            'stone_5_y',
            'stone_6_y',
            'stone_7_y',
            'stone_8_y',
            'stone_9_y',
            'stone_10_y',
            'stone_11_y',
            'stone_12_y']  
             
x_min, x_max = 0, 1500
y_min, y_max = 0, 3000

# Create masks column by column
x_ok = (df[x_cols] >= x_min) & (df[x_cols] <= x_max) | (df[x_cols] == 4095)
y_ok = (df[y_cols] >= y_min) & (df[y_cols] <= y_max) | (df[y_cols] == 4095)

# Rows where all x and y columns are okay
rows_ok = x_ok.all(axis=1) & y_ok.all(axis=1)

# Rows that are NOT ok
rows_out_of_range = df[~rows_ok]

print(rows_out_of_range)

```
All the stone coordinates are in range


```{python}
df.info()
```
```{python}
categorical_cols = [
    'competition_id',
    'end_id',
    'player_id',
    'task',
    'handle',
    'sheet',
    'lsfe',
    'winner',
    'powerplay',
    'timeout',
    'points'
]


for col in categorical_cols:
    freq_table = df[col].value_counts(dropna=False).reset_index()
    freq_table.columns = [col, 'Frequency']
    freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
    
    print(f"Frequency table for {col}:\n", freq_table, "\n")
```
```{python}
df_invalid_player = df[(df["player_id"] == 3)][["match_id","end_id"]]
print("player_id = 3:")
print(df_invalid_player)
print("---------------------")
df_invalid_points = df[(df["points"] > 4)][["match_id","end_id"]]
print("points > 4:")
print(df_invalid_points)
```
```{python}
match_id_to_view = '22230015_48_1'
df_match = df[df["match_id"] == match_id_to_view]
df_match = df_match.sort_values(by='shot_id')
count = 0
for (mid, eid), game in df_match.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","player_id","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```
```{python}
match_id_to_view = '0_9_2'
df_match = df[df["match_id"] == match_id_to_view]
df_match = df_match.sort_values(by='shot_id')
count = 0
for (mid, eid), game in df_match.groupby(['match_id', 'end_id']):
    print(f"Match {mid}, End {eid}")
    print(game[["team_id","points","result","task","shot_id"]])
    print("-" * 50)
    count += 1
    if count >= 10:
        break
```

EXPLORATION

We will explore the different types of shots
"0": "Draw"
"1": "Front"
"2": "Guard"
"3": "Raise / Tap-back"
"4": "Wick / Soft Peeling"
"5": "Freeze"
"6": "Take-out"
"7": "Hit and Roll"
"8": "Clearing"
"9": "Double Take-out"
"10": "Promotion Take-out"
"11": "through"
"13": "no statistics"
```{python}
freq_table = df['task'].value_counts().reset_index()
freq_table.columns = ['Task', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['Task'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Task Values')
plt.xlabel('Task')
plt.ylabel('Frequency')
```

Draw is the most common shot by far.
Freeze and Through are the least common.

How often players perfectly execute their shot?
```{python}
freq_table = df['points'].value_counts().reset_index()
freq_table.columns = ['Point', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['Point'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Point Values')
plt.xlabel('Point')
plt.ylabel('Frequency')
```
```{python}
freq = df.groupby(['end_id','hammer']).size().unstack(fill_value=0)
freq.columns = ['Non-Hammer','Hammer']

ends = freq.index

# Plot stacked bars
fig, ax = plt.subplots(figsize=(10,6))
ax.bar(ends, freq['Hammer'], label='Hammer', color='blue')
ax.bar(ends, freq['Non-Hammer'], bottom=freq['Hammer'], label='Non-Hammer', color='orange')

ax.set_xlabel('End ID')
ax.set_ylabel('Frequency')
ax.set_title('Hammer vs Non-Hammer Frequencies per End')
ax.legend()
plt.show()
```
```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Your average results (already aggregated)
avg_results = df.groupby(['end_id','hammer'])['result'].mean().reset_index()

ends = sorted(avg_results['end_id'].unique())
width = 0.35

fig, ax = plt.subplots(figsize=(10,6), layout='constrained')

# Plot bars for each hammer value
for i, hammer_val in enumerate([1,0]):  # 1=Hammer, 0=Non-Hammer
    hammer_data = avg_results[avg_results['hammer']==hammer_val]
    # Align bars by offset
    offset = (i - 0.5) * width  # i=0 -> left, i=1 -> right
    ax.bar(hammer_data['end_id'] + offset, hammer_data['result'], width, 
           label='Hammer' if hammer_val==1 else 'Non-Hammer')

ax.set_xlabel('End ID')
ax.set_ylabel('Average Result')
ax.set_title('Average Result per End: Hammer vs Non-Hammer')
ax.set_xticks(ends)
ax.legend()
plt.show()
```
```{python}
results = sorted(df['result'].unique())

# Compute frequencies
hammer_counts = df[df['hammer']==1]['result'].value_counts().reindex(results, fill_value=0)
nonhammer_counts = df[df['hammer']==0]['result'].value_counts().reindex(results, fill_value=0)

x = np.arange(len(results))

# Plot stacked bars
fig, ax = plt.subplots(figsize=(8,6))
ax.bar(x, hammer_counts, label='Hammer', color='blue')
ax.bar(x, nonhammer_counts, bottom=hammer_counts, label='Non-Hammer', color='orange')

ax.set_xticks(x)
ax.set_xticklabels(results)
ax.set_xlabel('Result')
ax.set_ylabel('Frequency')
ax.set_title('Stacked Frequency of Results by Hammer vs Non-Hammer')
ax.legend()
plt.show()
```
```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Task code to label mapping
task_labels = {
    0: "Draw",
    1: "Front",
    2: "Guard",
    3: "Raise / Tap-back",
    4: "Wick / Soft Peeling",
    5: "Freeze",
    6: "Take-out",
    7: "Hit and Roll",
    8: "Clearing",
    9: "Double Take-out",
    10: "Promotion Take-out",
    11: "Through",
    13: "No Statistics"
}

# Map selected tasks to labels; everything else will become 'Other'
selected_tasks = [0, 1,2,6, 8,]  # your chosen subset
df['task_grouped'] = df['task'].where(df['task'].isin(selected_tasks), 'Other')
df['task_label'] = df['task_grouped'].map(task_labels)
df['task_label'] = df['task_label'].fillna('Other')  # ensure 'Other' exists

# Function to compute percent table per hammer value
def get_percent_table(df_subset, task_order):
    freq = df_subset.groupby(['end_id','task_label']).size().unstack(fill_value=0)
    freq_percent = freq.div(freq.sum(axis=1), axis=0) * 100
    # order columns safely
    column_order = [t for t in task_order if t in freq_percent.columns] + ['Other']
    freq_percent = freq_percent[column_order]
    return freq_percent

# Define task order for plotting
task_order = [task_labels[i] for i in selected_tasks]

# Separate for hammer and non-hammer
hammer_df = get_percent_table(df[df['hammer']==1], task_order)
nonhammer_df = get_percent_table(df[df['hammer']==0], task_order)

# Plot Hammer heatmap
plt.figure(figsize=(12,6))
sns.heatmap(hammer_df, annot=True, fmt='.1f', cmap='Blues', cbar_kws={'label':'Percent'})
plt.xlabel('Shot Type')
plt.ylabel('End ID')
plt.title('Percent of Shot Types per End - Hammer')
plt.show()

# Plot Non-Hammer heatmap
plt.figure(figsize=(12,6))
sns.heatmap(nonhammer_df, annot=True, fmt='.1f', cmap='Oranges', cbar_kws={'label':'Percent'})
plt.xlabel('Shot Type')
plt.ylabel('End ID')
plt.title('Percent of Shot Types per End - Non-Hammer')
plt.show()

```


Logistic Regression
What factors influence whether or not the non-hammer team steals?

Steal = 1  if non hammer scores â‰¥ 1 (steal)
Steal = 0  if non hammer scores 0 
```{python}
df = df[df['hammer']==0]
df['steal'] = df.apply(lambda row: '1' if row['result'] >= 1
                                     else ('0' if row['result']==0 else None), axis=1)

```

```{python}
freq_table = df['steal'].value_counts(dropna=False).reset_index()
freq_table.columns = ['steal', 'Frequency']
freq_table['Percentage'] = (freq_table['Frequency'] / freq_table['Frequency'].sum() * 100).round(2)
print(freq_table)
bars = plt.bar(freq_table['steal'].astype(str), freq_table['Frequency'], color='skyblue')
plt.title('Frequency of Steal')
plt.xlabel('Non hammer steal')
plt.ylabel('Frequency')
```






```{python}
aggregate_features = []
categorical_features = ["powerplay", "task"]

all_features = stone_features + aggregate_features + categorical_features + ["shot_id", "hammer"]

```

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score
import pandas as pd
import numpy as np

# --- Train-test split ---
X_train, X_test, y_train, y_test = train_test_split(
    df_hammer[all_features],
    df_hammer["steal"],
    test_size=0.2,
    random_state=42,
    stratify=df_hammer["steal"]
)

# --- Preprocessing ---
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), categorical_features)
    ]
)

# --- Define classifiers ---
classifiers = {
    "Logistic L2": LogisticRegressionCV(
        Cs=20,
        penalty="l2",
        solver="saga",
        cv=5,
        scoring="roc_auc",
        max_iter=5000,
        class_weight='balanced',
        n_jobs=-1
    ),
    "Random Forest": RandomForestClassifier(
        n_estimators=500,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42
    ),
    "XGBoost": XGBClassifier(
        n_estimators=500,
        max_depth=3,
        learning_rate=0.05,
        scale_pos_weight=(y_train==0).sum() / (y_train==1).sum(),
        eval_metric='auc',
        n_jobs=-1,
        random_state=42
    )
}

# --- Loop through classifiers ---
for name, clf in classifiers.items():
    print(f"\n=== {name} ===")
    pipeline = Pipeline([
        ("preprocess", preprocessor),
        ("clf", clf)
    ])
    
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    y_prob = pipeline.predict_proba(X_test)[:,1]
    
    # Metrics
    print("ROC-AUC:", roc_auc_score(y_test, y_prob))
    print(classification_report(y_test, y_pred))
    
    # Feature importance / coefficients
    feature_names = list(numeric_features) + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))
    
    if name == "Logistic L1":
        coefs = pipeline.named_steps['clf'].coef_[0]
        coef_df = pd.Series(coefs, index=feature_names).sort_values(key=abs, ascending=False)
        print("Top non-zero features:")
        print(coef_df[coef_df!=0].head(20))
        
    elif name in ["Random Forest", "XGBoost"]:
        importances = pipeline.named_steps['clf'].feature_importances_
        imp_df = pd.Series(importances, index=feature_names).sort_values(ascending=False)
        print("Top 20 features by importance:")
        print(imp_df.head(20))


```












